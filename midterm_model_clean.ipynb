{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAEN 429 Midterm Bonus\n",
    "### Fletcher Newman | October 14th, 2025\n",
    "This is the clean file with only the neural network training code and compute_loss() function\n",
    "\n",
    "Upload `test.csv` in the same working directory to get MSE loss\n",
    "\n",
    "Regularization used:\n",
    "- Dropout (3 random features each layer)\n",
    "- Early stopping (patience of 10 epochs)\n",
    "\n",
    "My work in testing different models and going through a small amount of EDA is shown in the `midterm_model_scratchWork.ipynb` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test.csv not in directory, defaulting to Null\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "train_df = pd.read_csv('BonusQuestionTrain.csv')\n",
    "val_df = pd.read_csv('BonusQuestionVal.csv')\n",
    "\n",
    "# Assuming test dataset will be put into directory\n",
    "if os.path.isfile('test.csv'):\n",
    "    test = pd.read_csv('test.csv')\n",
    "else:\n",
    "    print('test.csv not in directory, defaulting to Null')\n",
    "    test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Multi-Layer Neural Network with Regularization\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer neural network for non-linear regression.\n",
    "    \n",
    "    Architecture:\n",
    "    - Input layer: 10 features\n",
    "    - Hidden layer 1: 128 neurons + BatchNorm + ReLU + Dropout\n",
    "    - Hidden layer 2: 64 neurons + BatchNorm + ReLU + Dropout\n",
    "    - Hidden layer 3: 32 neurons + BatchNorm + ReLU + Dropout\n",
    "    - Output layer: 1 neuron (regression output)\n",
    "    \n",
    "    Regularization techniques:\n",
    "    - Dropout (0.3): Randomly zeros some neurons during training to prevent overfitting\n",
    "    - Batch Normalization: Normalizes layer inputs for stable training\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size=10, dropout_rate=0.3):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        # Layer 1\n",
    "        self.fc1 = nn.Linear(input_size, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Layer 2\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Layer 3\n",
    "        self.fc3 = nn.Linear(128, 32)\n",
    "        self.bn3 = nn.BatchNorm1d(32)\n",
    "        self.dropout3 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc4 = nn.Linear(32, 1)\n",
    "        \n",
    "        # Activation function\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Layer 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        # Layer 3\n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        # Output layer (no activation for regression)\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function\n",
    "def train_nn(model, train_df):\n",
    "    \"\"\"Trains neural net\"\"\"\n",
    "    # Training Configuration\n",
    "    # Loss function: Mean Squared Error (MSE) for regression\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    # Optimizer: Adam with weight decay (L2 regularization)\n",
    "    # Weight decay adds penalty for large weights, preventing overfitting\n",
    "    learning_rate = 0.001\n",
    "    weight_decay = 1e-5  # L2 regularization strength\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "    # Prepare data\n",
    "    # Separate features and target\n",
    "    X_train = train_df.drop('y', axis=1).values\n",
    "    y_train = train_df['y'].values\n",
    "\n",
    "    # Standardize features (important for neural networks!)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.FloatTensor(X_train_scaled)\n",
    "    y_train_tensor = torch.FloatTensor(y_train).reshape(-1, 1)\n",
    "\n",
    "    # Training Loop\n",
    "    num_epochs = 100\n",
    "    batch_size = 32\n",
    "    patience = 10  \n",
    "    min_delta = 0.0001  \n",
    "    best_train_loss = float('inf')\n",
    "    epochs_without_improvement = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    # Create data loaders for batch training\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Track training and validation losses\n",
    "    train_losses = []\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Epochs: {num_epochs}, Batch size: {batch_size}\\n\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train() \n",
    "        \n",
    "        for batch_X, batch_y in train_loader:\n",
    "            # Forward pass\n",
    "            predictions = model(batch_X)\n",
    "            loss = criterion(predictions, batch_y)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()  # Clear previous gradients\n",
    "            loss.backward()        # Compute gradients\n",
    "            optimizer.step()       # Update weights\n",
    "\n",
    "        # Compute training loss WITHOUT dropout (for fair comparison)\n",
    "        model.eval()  # Disable dropout for evaluation\n",
    "        with torch.no_grad():\n",
    "            train_predictions = model(X_train_tensor)\n",
    "            train_loss = criterion(train_predictions, y_train_tensor)\n",
    "            train_losses.append(train_loss.item())\n",
    "\n",
    "        \n",
    "        # Early stopping check\n",
    "        if train_loss.item() < best_train_loss - min_delta:\n",
    "            # Validation loss improved\n",
    "            best_train_loss = train_loss.item()\n",
    "            epochs_without_improvement = 0\n",
    "            # Save the best model state (deep copy)\n",
    "            best_model_state = OrderedDict({k: v.clone() for k, v in model.state_dict().items()})\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss.item():.4f} ✓ (New best)\")\n",
    "        else:\n",
    "            # No improvement\n",
    "            epochs_without_improvement += 1\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}] - Train Loss: {train_loss.item():.4f} (No improvement: {epochs_without_improvement}/{patience})\")\n",
    "            \n",
    "            # Check if we should stop\n",
    "            if epochs_without_improvement >= patience:\n",
    "                print(f\"\\nEarly stopping triggered after {epoch+1} epochs!\")\n",
    "                print(f\"Best train loss: {best_train_loss:.4f}\")\n",
    "                break\n",
    "\n",
    "    # Restore the best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(\"\\nRestored model to best train loss state\")\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "    print(f\"Final Training Loss: {train_losses[-1 - epochs_without_improvement]:.4f}\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(model, df):\n",
    "    \"\"\"\n",
    "    Compute MSE loss on test data.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        test: pandas DataFrame with same format as training data\n",
    "    \n",
    "    Returns:\n",
    "        MSE\n",
    "    \"\"\"\n",
    "    # Check if df was imported\n",
    "    if df is None:\n",
    "        print('ERROR: Dataframe was not imported (defaulted to null)')\n",
    "        return \n",
    "    \n",
    "    # Extract features and target\n",
    "    X_test = df.drop('y', axis=1).values\n",
    "    y_test = df['y'].values\n",
    "    \n",
    "    # Standardize using the same scaler from training\n",
    "    scaler = StandardScaler()\n",
    "    X_test_scaled = scaler.fit_transform(X_test)\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_test_tensor = torch.FloatTensor(X_test_scaled)\n",
    "    y_test_tensor = torch.FloatTensor(y_test).reshape(-1, 1)\n",
    "\n",
    "    # MSE loss\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Compute predictions\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor)\n",
    "        mse_loss = criterion(predictions, y_test_tensor)\n",
    "    \n",
    "    # Print MSE loss as required\n",
    "    print(f\"MSE Loss on test data: {mse_loss.item():.6f}\")\n",
    "    \n",
    "    return mse_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Init and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epochs: 100, Batch size: 32\n",
      "\n",
      "Epoch [1/100] - Train Loss: 3.9890 ✓ (New best)\n",
      "Epoch [2/100] - Train Loss: 1.7067 ✓ (New best)\n",
      "Epoch [3/100] - Train Loss: 0.6566 ✓ (New best)\n",
      "Epoch [4/100] - Train Loss: 0.6469 ✓ (New best)\n",
      "Epoch [5/100] - Train Loss: 0.3600 ✓ (New best)\n",
      "Epoch [6/100] - Train Loss: 0.5488 (No improvement: 1/10)\n",
      "Epoch [7/100] - Train Loss: 0.5686 (No improvement: 2/10)\n",
      "Epoch [8/100] - Train Loss: 0.4217 (No improvement: 3/10)\n",
      "Epoch [9/100] - Train Loss: 0.4875 (No improvement: 4/10)\n",
      "Epoch [10/100] - Train Loss: 0.4258 (No improvement: 5/10)\n",
      "Epoch [11/100] - Train Loss: 0.6163 (No improvement: 6/10)\n",
      "Epoch [12/100] - Train Loss: 0.4746 (No improvement: 7/10)\n",
      "Epoch [13/100] - Train Loss: 0.5572 (No improvement: 8/10)\n",
      "Epoch [14/100] - Train Loss: 0.3213 ✓ (New best)\n",
      "Epoch [15/100] - Train Loss: 0.4276 (No improvement: 1/10)\n",
      "Epoch [16/100] - Train Loss: 0.3246 (No improvement: 2/10)\n",
      "Epoch [17/100] - Train Loss: 0.3498 (No improvement: 3/10)\n",
      "Epoch [18/100] - Train Loss: 0.6374 (No improvement: 4/10)\n",
      "Epoch [19/100] - Train Loss: 0.3558 (No improvement: 5/10)\n",
      "Epoch [20/100] - Train Loss: 0.4515 (No improvement: 6/10)\n",
      "Epoch [21/100] - Train Loss: 0.3741 (No improvement: 7/10)\n",
      "Epoch [22/100] - Train Loss: 0.3164 ✓ (New best)\n",
      "Epoch [23/100] - Train Loss: 0.5344 (No improvement: 1/10)\n",
      "Epoch [24/100] - Train Loss: 0.3576 (No improvement: 2/10)\n",
      "Epoch [25/100] - Train Loss: 0.2810 ✓ (New best)\n",
      "Epoch [26/100] - Train Loss: 0.3714 (No improvement: 1/10)\n",
      "Epoch [27/100] - Train Loss: 0.4081 (No improvement: 2/10)\n",
      "Epoch [28/100] - Train Loss: 0.2682 ✓ (New best)\n",
      "Epoch [29/100] - Train Loss: 0.2490 ✓ (New best)\n",
      "Epoch [30/100] - Train Loss: 0.1850 ✓ (New best)\n",
      "Epoch [31/100] - Train Loss: 0.3436 (No improvement: 1/10)\n",
      "Epoch [32/100] - Train Loss: 0.4215 (No improvement: 2/10)\n",
      "Epoch [33/100] - Train Loss: 0.3092 (No improvement: 3/10)\n",
      "Epoch [34/100] - Train Loss: 0.2557 (No improvement: 4/10)\n",
      "Epoch [35/100] - Train Loss: 0.2176 (No improvement: 5/10)\n",
      "Epoch [36/100] - Train Loss: 0.4134 (No improvement: 6/10)\n",
      "Epoch [37/100] - Train Loss: 0.3308 (No improvement: 7/10)\n",
      "Epoch [38/100] - Train Loss: 0.4259 (No improvement: 8/10)\n",
      "Epoch [39/100] - Train Loss: 0.3943 (No improvement: 9/10)\n",
      "Epoch [40/100] - Train Loss: 0.2466 (No improvement: 10/10)\n",
      "\n",
      "Early stopping triggered after 40 epochs!\n",
      "Best train loss: 0.1850\n",
      "\n",
      "Restored model to best train loss state\n",
      "\n",
      "Training completed!\n",
      "Final Training Loss: 0.1850\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = NeuralNetwork(input_size=10, dropout_rate=0.3)\n",
    "\n",
    "# Combine training and validation for more training data\n",
    "# They were used seperately in the scratch work notebook\n",
    "mega_df = pd.concat([train_df, val_df], ignore_index=True)\n",
    "\n",
    "# Train model\n",
    "best_model = train_nn(model, mega_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate loss for test df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Dataframe was not imported (defaulted to null)\n"
     ]
    }
   ],
   "source": [
    "test_loss = compute_loss(best_model, test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
