{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAEN 429 Homework 7\n",
    "Fletcher Newman | fletcht13@tmau.edu | November 4th, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn(sequence):\n",
    "    \"\"\"\n",
    "    input is a list of R^4 vectors\n",
    "    Represents different tokens being put into the model\n",
    "    Input dim: 4\n",
    "    \"\"\"\n",
    "    # Weights are not being trained so we hard code them\n",
    "    W = torch.tensor([[1, 0, -1, 1, 1, 0, 1, 1],\n",
    "                           [0, 2, 1, 0, 0, 1, 1, -1],\n",
    "                           [1, 2, 1, 0, 0, 1, 0, -1]], \n",
    "                           dtype=torch.float32)\n",
    "\n",
    "    W_y = torch.tensor([3, 0, -1, 3], dtype=torch.float32)\n",
    "\n",
    "    # Init hidden state\n",
    "    H = torch.tensor([0, 0, 0, 1], dtype=torch.float32)\n",
    "\n",
    "    # Init output \n",
    "    output = []\n",
    "\n",
    "    # Loop through sequence\n",
    "    for x in sequence:\n",
    "        # Concat vecs to do full matrix mult\n",
    "        full_vec = torch.cat([x, H], dim=0)\n",
    "\n",
    "        # Get feature value\n",
    "        F = torch.matmul(W, full_vec)\n",
    "\n",
    "        # Update H\n",
    "        H = torch.tanh(F)\n",
    "        # Add bias 1\n",
    "        H = torch.cat([H, torch.tensor([1.0], dtype=H.dtype)], dim=0)\n",
    "\n",
    "        # Multiply weights for y\n",
    "        O = torch.matmul(W_y, H)\n",
    "        # Pass through sigmoid\n",
    "        y = torch.sigmoid(O)\n",
    "\n",
    "        # Add to output\n",
    "        output.append(y)\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor(0.9988), tensor(0.9960), tensor(0.9945), tensor(0.9933)]\n"
     ]
    }
   ],
   "source": [
    "# Ex sequence\n",
    "x1 = [2, -1, 0, 1]\n",
    "x2 = [1, 0, 1, 0]\n",
    "x3 = [1, 1, 0, 0]\n",
    "x4 = [1, 2, -1, 2]\n",
    "\n",
    "X = torch.tensor([x1, x2, x3, x4], dtype=torch.float32)\n",
    "\n",
    "Y = rnn(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM(X):\n",
    "    \"\"\"\n",
    "    Sequence length: 4\n",
    "    Input dim: 3\n",
    "    Hidden state dim: 2\n",
    "    \"\"\"\n",
    "\n",
    "    # init forget gate\n",
    "    FG = torch.tensor([[0, -1, 0, 0, -2, 0],\n",
    "                       [-2, 0, -2, -2, 0, 0]], dtype=torch.float32)\n",
    "\n",
    "    # Init Candidate memory\n",
    "    CM = torch.tensor([[0, 1, 0, 0, -1, 0],\n",
    "                       [-1, 0, -1, -4, 0, 1]], dtype=torch.float32)\n",
    "\n",
    "    # Init Input gate\n",
    "    IG = torch.tensor([[2, 0, 3, 1, 0, 1],\n",
    "                       [0, 1, 0, 0, 2, 0]], dtype=torch.float32)\n",
    "\n",
    "    # Init output gate\n",
    "    OG = torch.tensor([[1, 1, 0, 4, 1, 0],\n",
    "                       [0, 0, -1, -1, -2, 4]], dtype=torch.float32)\n",
    "    \n",
    "    # Set initial hidden state (short term memory)\n",
    "    H = torch.tensor([0, 0, 1], dtype=torch.float32)\n",
    "\n",
    "    # Set initial long term memory\n",
    "    LM = torch.tensor([0, 0])\n",
    "\n",
    "    # Init output list \n",
    "    output = []\n",
    "\n",
    "    # Loop through tokens\n",
    "    for x in X:\n",
    "        # Concat hidden state\n",
    "        x_H = torch.cat([x, H], dim=0)\n",
    "\n",
    "        # Perform matrix operations with each weight matrix\n",
    "        x_FG = torch.sigmoid(torch.matmul(FG, x_H))\n",
    "        x_CM = torch.tanh(torch.matmul(CM, x_H))\n",
    "        x_IG = torch.sigmoid(torch.matmul(IG, x_H))\n",
    "\n",
    "        # Multiply and add to update long term memory \n",
    "        LM = (LM*x_FG) + (x_CM*x_IG)\n",
    "\n",
    "        # Take tan hedge\n",
    "        LM_tanh = torch.tanh(LM)\n",
    "\n",
    "        # Multiply with output gate sigmoid to update hidden state\n",
    "        x_OG = torch.sigmoid(torch.matmul(OG, x_H))\n",
    "        H = LM_tanh * x_OG\n",
    "\n",
    "        # Appened to output list\n",
    "        output.append(H)\n",
    "\n",
    "        # Add bias term back\n",
    "        H = torch.cat([H, torch.tensor([1.0])])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([ 0.7097, -0.5577]), tensor([ 0.8852, -0.4358]), tensor([ 0.5500, -0.0967]), tensor([ 0.3415, -0.3512])]\n"
     ]
    }
   ],
   "source": [
    "# Ex sequence\n",
    "x1 = [1, 2, 1]\n",
    "x2 = [2, 1, 0]\n",
    "x3 = [0, -1, 3]\n",
    "x4 = [-1, 0, 2]\n",
    "\n",
    "X = torch.tensor([x1, x2, x3, x4]) #, dtype=torch.float32)\n",
    "\n",
    "Y = LSTM(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def self_attention(X):\n",
    "    \"\"\"\n",
    "    Takes in 6x4 matrix \n",
    "    Columns of X repressent tokens\n",
    "    Output is attention weighted features matrix\n",
    "    \"\"\"\n",
    "    # Define W_Q\n",
    "    W_Q = torch.tensor([[1, 1, 0, 0, 0, 0],\n",
    "                       [0, 1, 0, 1, 0, 0],\n",
    "                       [0, 0, 1, 0, 1, 1]], dtype=torch.float32)\n",
    "    \n",
    "    # Define W_K\n",
    "    W_K = torch.tensor([[0, 0, 1, 0, 0, 0],\n",
    "                       [0, 1, 0, 0, 0, 0],\n",
    "                       [1, 0, 0, 0, 0, -1]], dtype=torch.float32)\n",
    "    \n",
    "    # Define W_V\n",
    "    W_V = torch.tensor([[10, 0, 0, 0, 0, 0],\n",
    "                       [0, 0, 0, 10, 0, 0],\n",
    "                       [0, 10, 0, 0, 0, 0]], dtype=torch.float32)\n",
    "    \n",
    "    # Matrix mult operations\n",
    "    Q = torch.matmul(W_Q, X)\n",
    "    K = torch.matmul(W_K, X)\n",
    "    \n",
    "    # Get key dimesion (just 3 for this implementation)\n",
    "    k = W_K.size(0)\n",
    "    \n",
    "    # Matrix multiply W_Q and W_K and divide by k to scale\n",
    "    r = torch.matmul(K.T, Q) / np.sqrt(k)\n",
    "\n",
    "    # Take the softmax to get attention weight matrix\n",
    "    A = torch.softmax(r, dim=0)\n",
    "\n",
    "    # Get attention weighted feature embeddings\n",
    "    V = torch.matmul(W_V, X)\n",
    "    Z = torch.matmul(V, A)\n",
    "\n",
    "    return Z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10.3076, 10.1055, 15.0336,  3.0608],\n",
      "        [ 2.8328,  2.9733,  4.1317,  1.5304],\n",
      "        [ 4.5903,  4.5003,  2.1099,  7.7044]])\n"
     ]
    }
   ],
   "source": [
    "# Ex sequence\n",
    "x1 = [2, 0, 0, 0, 2, 1]\n",
    "x2 = [0, 1, 2, 0, 0, 0]\n",
    "x3 = [0, 0, 1, 1, 0, 1]\n",
    "x4 = [2, 0, 0, 1, 0, 1]\n",
    "\n",
    "X = torch.tensor([x1, x2, x3, x4], dtype=torch.float32).T\n",
    "\n",
    "Z = self_attention(X)\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2\n",
    "Compute perplexity:\n",
    "\n",
    "$\\exp(-\\frac{1}{n}\\sum \\log P(x_t|x_{t-1}, ... ,x_1))$\n",
    "\n",
    "In this problem, we have:\n",
    "\n",
    "$\\exp(-\\frac{1}{5}\\sum [\\log(0.45)+\\log(0.1)+\\log(0.2)+\\log(0.1)+\\log(0.4)])$\n",
    "\n",
    "$=\\exp(-\\frac{1}{5}(-7.9294))=\\exp(1.5859)=4.8836$\n",
    "\n",
    "So we have\n",
    "\n",
    "$Perplexity=4.8836$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
